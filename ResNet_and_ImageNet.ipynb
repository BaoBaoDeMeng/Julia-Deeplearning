{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet and ImageNet\n",
    "\n",
    "在该实现中您可以看到如下功能：\n",
    "1. 读取 ImageFolder 并进行预处理，切分 batch\n",
    "2. 模型的读入和保存\n",
    "3. 对模型的训练和测试的封装\n",
    "\n",
    "In this template you can finish the following functions:\n",
    "1. Read ImageFolder and pre-process it, divide it into batches\n",
    "2. Reading and saving the model\n",
    "3. Encapsulation of model training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, Metalhead, Statistics\n",
    "using Flux: onehotbatch, onecold, logitcrossentropy, throttle, flatten\n",
    "using Metalhead: trainimgs\n",
    "using Parameters: @with_kw\n",
    "using Images: channelview\n",
    "using Statistics: mean\n",
    "using Base.Iterators: partition\n",
    "using CUDAapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training on GPU-0\n",
      "└ @ Main In[2]:7\n"
     ]
    }
   ],
   "source": [
    "using CUDAapi, CUDAdrv, CUDAnative\n",
    "gpu_id = 0  ## set < 0 for no cuda, >= 0 for using a specific device (if available)\n",
    "\n",
    "if has_cuda_gpu() && gpu_id >=0\n",
    "    device!(gpu_id)\n",
    "    device = Flux.gpu\n",
    "    @info \"Training on GPU-$(gpu_id)\"\n",
    "else\n",
    "    device = Flux.cpu\n",
    "    @info \"Training on CPU\"\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Args"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Parameters: @with_kw\n",
    "@with_kw mutable struct Args\n",
    "    batch_size::Int = 64\n",
    "    lr::Float64 = 5e-5\n",
    "    epochs::Int = 10\n",
    "    patience::Int = 5\n",
    "    data_workers::Int = 4\n",
    "    train_data_dir::String = \"/home/zhangzhi/Data/ImageNet2012/train\"\n",
    "    val_data_dir::String = \"/home/zhangzhi/Data/ImageNet2012/train\"\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Args\n",
       "  batch_size: Int64 64\n",
       "  lr: Float64 5.0e-5\n",
       "  epochs: Int64 10\n",
       "  patience: Int64 5\n",
       "  data_workers: Int64 4\n",
       "  train_data_dir: String \"/home/zhangzhi/Data/ImageNet2012/train\"\n",
       "  val_data_dir: String \"/home/zhangzhi/Data/ImageNet2012/train\"\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模仿 pytorch 使用多个 worker 读取数据集，进行预处理，并且分为n个batch。这里将其封装为单独的文件。\n",
    "\n",
    "Imitating pytorch, we uses multiple workers to read the data set, preprocess it, and divide it into n batches. Here it is packaged as a separate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"dataset.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Adding 4 new data workers...\n",
      "└ @ Main /data/zhangzhi/julia/dataset.jl:190\n",
      "┌ Info: Adding 4 new data workers...\n",
      "└ @ Main /data/zhangzhi/julia/dataset.jl:190\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ImagenetDataset(\"/home/zhangzhi/Data/ImageNet2012/train\", 64, imagenet_val_data_loader, [\"n01440764/n01440764_10026.JPEG\", \"n01440764/n01440764_10027.JPEG\", \"n01440764/n01440764_10029.JPEG\", \"n01440764/n01440764_10040.JPEG\", \"n01440764/n01440764_10042.JPEG\", \"n01440764/n01440764_10043.JPEG\", \"n01440764/n01440764_10048.JPEG\", \"n01440764/n01440764_10066.JPEG\", \"n01440764/n01440764_10074.JPEG\", \"n01440764/n01440764_1009.JPEG\"  …  \"n15075141/n15075141_9816.JPEG\", \"n15075141/n15075141_9819.JPEG\", \"n15075141/n15075141_9835.JPEG\", \"n15075141/n15075141_9855.JPEG\", \"n15075141/n15075141_9907.JPEG\", \"n15075141/n15075141_9915.JPEG\", \"n15075141/n15075141_9933.JPEG\", \"n15075141/n15075141_9942.JPEG\", \"n15075141/n15075141_999.JPEG\", \"n15075141/n15075141_9993.JPEG\"], QueuePool([6, 7, 8, 9], RemoteChannel{Channel{Tuple}}(1, 1, 29), RemoteChannel{Channel{Tuple}}(1, 1, 30), RemoteChannel{Channel{Bool}}(1, 1, 31), 0, Dict{Int64,Any}()))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = ImagenetDataset(args.train_data_dir, args.data_workers, args.batch_size, imagenet_train_data_loader)\n",
    "val_dataset = ImagenetDataset(args.val_data_dir, args.data_workers, args.batch_size, imagenet_val_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义 ResNet。\n",
    "\n",
    "Define ResNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Chain(Conv((7, 7), 3=>64), MaxPool((3, 3), pad = (1, 1), stride = (2, 2)), Metalhead.ResidualBlock((Conv((1, 1), 64=>64), Conv((3, 3), 64=>64), Conv((1, 1), 64=>256)), (BatchNorm(64), BatchNorm(64), BatchNorm(256)), Chain(Conv((1, 1), 64=>256), BatchNorm(256))), Metalhead.ResidualBlock((Conv((1, 1), 256=>64), Conv((3, 3), 64=>64), Conv((1, 1), 64=>256)), (BatchNorm(64), BatchNorm(64), BatchNorm(256)), identity), Metalhead.ResidualBlock((Conv((1, 1), 256=>64), Conv((3, 3), 64=>64), Conv((1, 1), 64=>256)), (BatchNorm(64), BatchNorm(64), BatchNorm(256)), identity), Metalhead.ResidualBlock((Conv((1, 1), 256=>128), Conv((3, 3), 128=>128), Conv((1, 1), 128=>512)), (BatchNorm(128), BatchNorm(128), BatchNorm(512)), Chain(Conv((1, 1), 256=>512), BatchNorm(512))), Metalhead.ResidualBlock((Conv((1, 1), 512=>128), Conv((3, 3), 128=>128), Conv((1, 1), 128=>512)), (BatchNorm(128), BatchNorm(128), BatchNorm(512)), identity), Metalhead.ResidualBlock((Conv((1, 1), 512=>128), Conv((3, 3), 128=>128), Conv((1, 1), 128=>512)), (BatchNorm(128), BatchNorm(128), BatchNorm(512)), identity), Metalhead.ResidualBlock((Conv((1, 1), 512=>128), Conv((3, 3), 128=>128), Conv((1, 1), 128=>512)), (BatchNorm(128), BatchNorm(128), BatchNorm(512)), identity), Metalhead.ResidualBlock((Conv((1, 1), 512=>256), Conv((3, 3), 256=>256), Conv((1, 1), 256=>1024)), (BatchNorm(256), BatchNorm(256), BatchNorm(1024)), Chain(Conv((1, 1), 512=>1024), BatchNorm(1024))), Metalhead.ResidualBlock((Conv((1, 1), 1024=>256), Conv((3, 3), 256=>256), Conv((1, 1), 256=>1024)), (BatchNorm(256), BatchNorm(256), BatchNorm(1024)), identity), Metalhead.ResidualBlock((Conv((1, 1), 1024=>256), Conv((3, 3), 256=>256), Conv((1, 1), 256=>1024)), (BatchNorm(256), BatchNorm(256), BatchNorm(1024)), identity), Metalhead.ResidualBlock((Conv((1, 1), 1024=>256), Conv((3, 3), 256=>256), Conv((1, 1), 256=>1024)), (BatchNorm(256), BatchNorm(256), BatchNorm(1024)), identity), Metalhead.ResidualBlock((Conv((1, 1), 1024=>256), Conv((3, 3), 256=>256), Conv((1, 1), 256=>1024)), (BatchNorm(256), BatchNorm(256), BatchNorm(1024)), identity), Metalhead.ResidualBlock((Conv((1, 1), 1024=>256), Conv((3, 3), 256=>256), Conv((1, 1), 256=>1024)), (BatchNorm(256), BatchNorm(256), BatchNorm(1024)), identity), Metalhead.ResidualBlock((Conv((1, 1), 1024=>512), Conv((3, 3), 512=>512), Conv((1, 1), 512=>2048)), (BatchNorm(512), BatchNorm(512), BatchNorm(2048)), Chain(Conv((1, 1), 1024=>2048), BatchNorm(2048))), Metalhead.ResidualBlock((Conv((1, 1), 2048=>512), Conv((3, 3), 512=>512), Conv((1, 1), 512=>2048)), (BatchNorm(512), BatchNorm(512), BatchNorm(2048)), identity), Metalhead.ResidualBlock((Conv((1, 1), 2048=>512), Conv((3, 3), 512=>512), Conv((1, 1), 512=>2048)), (BatchNorm(512), BatchNorm(512), BatchNorm(2048)), identity), MeanPool((7, 7), pad = (0, 0, 0, 0), stride = (7, 7)), #103, Dense(2048, 1000)),)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Metalhead\n",
    "\n",
    "resnet = ResNet()\n",
    "model = Chain(resnet.layers[1:end-1]) |> device\n",
    "Flux.trainmode!(model, true)\n",
    "opt = ADAM(args.lr)\n",
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对模型的训练和测试的封装。\n",
    "\n",
    "Encapsulation of model training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using BSON\n",
    "using Tracker\n",
    "using Statistics, Printf\n",
    "using Flux.Optimise\n",
    "\n",
    "function save_model(model, filename)\n",
    "    model_state = Dict(\n",
    "        :weights => Tracker.data.(params(model))\n",
    "    )\n",
    "    open(filename, \"w\") do io\n",
    "        BSON.bson(io, model_state)\n",
    "    end\n",
    "end\n",
    "\n",
    "function load_model!(model, filename)\n",
    "    weights = BSON.load(filename)[:weights]\n",
    "    Flux.loadparams!(model, weights)\n",
    "    return model\n",
    "end\n",
    "\n",
    "@with_kw mutable struct State\n",
    "    epoch::Int = 1\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "end\n",
    "\n",
    "state = State()\n",
    "\n",
    "process_minibatch = (model, opt, x, y) -> begin\n",
    "    x = x |> device\n",
    "    y = y |> device\n",
    "    #@show model_to_host(y_hat)\n",
    "    #@show model_to_host(y)\n",
    "    loss(x, y) = logitcrossentropy(model(x), y)\n",
    "    Flux.train!(loss, params(model), [(x, y)], opt)\n",
    "    batch_loss = logitcrossentropy(model(x), y)\n",
    "    @show batch_loss\n",
    "    return Tracker.data(batch_loss |> cpu)\n",
    "end\n",
    "\n",
    "\n",
    "function train_epoch(model, opt)\n",
    "    # Clear out any previous training loss history\n",
    "    while length(state.train_loss_history) < state.epoch\n",
    "        push!(state.train_loss_history, Float64[])\n",
    "    end\n",
    "    state.train_loss_history[state.epoch] = zeros(Float64, length(train_dataset))\n",
    "\n",
    "    batch_idx = 1\n",
    "    avg_batch_time = 0.0\n",
    "    t_last = time()\n",
    "    for (x, y) in train_dataset\n",
    "        # Store training loss into loss history\n",
    "        state.train_loss_history[state.epoch][batch_idx] = process_minibatch(model, opt, x, y)\n",
    "\n",
    "        # Update average batch time\n",
    "        t_now = time()\n",
    "        avg_batch_time = .99*avg_batch_time + .01*(t_now - t_last)\n",
    "        t_last = t_now\n",
    "\n",
    "        # Calculate ETA\n",
    "        time_left = avg_batch_time*(length(train_dataset) - batch_idx)\n",
    "        hours = floor(Int,time_left/(60*60))\n",
    "        minutes = floor(Int, (time_left - hours*60*60)/60)\n",
    "        seconds = time_left - hours*60*60 - minutes*60\n",
    "        eta = @sprintf(\"%dh%dm%ds\", hours, minutes, seconds)\n",
    "\n",
    "        # Show a smoothed loss approximation per-minibatch\n",
    "        smoothed_loss = mean(state.train_loss_history[state.epoch][max(batch_idx-50,1):batch_idx])\n",
    "        println(@sprintf(\n",
    "            \"[TRAIN %d - %d/%d]: avg loss: %.4f, avg time: %.2fs, ETA: %s \",\n",
    "            state.epoch, batch_idx, length(train_dataset), smoothed_loss,\n",
    "            avg_batch_time,  eta,\n",
    "        ))\n",
    "\n",
    "        batch_idx += 1\n",
    "    end\n",
    "end\n",
    "\n",
    "function validate(model)\n",
    "    # Get the \"fast model\", \n",
    "    fast_model = Flux.mapleaves(Tracker.data, model)\n",
    "    Flux.testmode!(fast_model, true)\n",
    "\n",
    "    avg_loss = 0\n",
    "    batch_idx = 1\n",
    "    for (x, y) in val_dataset\n",
    "        # Push x through our fast model and calculate loss\n",
    "        y_hat = fast_model(x)\n",
    "        avg_loss += cpu(Flux.crossentropy(y_hat, y))\n",
    "\n",
    "        print(@sprintf(\n",
    "            \"\\r[VAL %d - %d/%d]: %.2f\",\n",
    "            state.epoch, batch_idx, length(val_dataset), avg_loss/batch_idx,\n",
    "        ))\n",
    "        batch_idx += 1\n",
    "    end\n",
    "    avg_loss /= length(val_dataset)\n",
    "    push!(state.val_loss_history, avg_loss)\n",
    "\n",
    "    # Return the average loss for this epoch\n",
    "    return avg_loss\n",
    "end\n",
    "\n",
    "\n",
    "function train(model, opt)\n",
    "    # Initialize best_epoch to epoch 0, with Infinity loss\n",
    "    best_epoch = (0, Inf)\n",
    "\n",
    "    while state.epoch < args.epochs\n",
    "        # Early-stop if we don't improve after `args.patience` epochs\n",
    "        if state.epoch > best_epoch[1] + args.patience\n",
    "            @info(\"Losing patience at epoch $(state.epoch)!\")\n",
    "            break\n",
    "        end\n",
    "\n",
    "        # Train for an epoch\n",
    "        train_epoch(model, opt)\n",
    "        \n",
    "        # Validate to see how much we've improved\n",
    "        epoch_loss = validate(model)\n",
    "\n",
    "        # Check to see if this epoch is the best we've seen so far\n",
    "        if epoch_loss < best_epoch[2]\n",
    "            best_epoch = (state.epoch, epoch_loss)\n",
    "        end\n",
    "\n",
    "        # Save our training state every epoch (but only save the model weights\n",
    "        # if this was the best epoch yet)\n",
    "        state.epoch += 1\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Beginning training run...\n",
      "└ @ Main In[9]:2\n",
      "┌ Info: Creating IIS with 1281167 images\n",
      "└ @ Main /data/zhangzhi/julia/dataset.jl:212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss = 6.9622507f0\n",
      "[TRAIN 1 - 1/20018]: avg loss: 6.9623, avg time: 0.78s, ETA: 4h21m11s \n",
      "batch_loss = 6.9277363f0\n",
      "[TRAIN 1 - 2/20018]: avg loss: 6.9450, avg time: 0.81s, ETA: 4h30m20s \n",
      "batch_loss = 6.942004f0\n",
      "[TRAIN 1 - 3/20018]: avg loss: 6.9440, avg time: 0.84s, ETA: 4h38m37s \n",
      "batch_loss = 6.9768744f0\n",
      "[TRAIN 1 - 4/20018]: avg loss: 6.9522, avg time: 0.86s, ETA: 4h45m42s \n",
      "batch_loss = 6.912876f0\n",
      "[TRAIN 1 - 5/20018]: avg loss: 6.9443, avg time: 0.87s, ETA: 4h51m37s \n",
      "batch_loss = 7.0515375f0\n",
      "[TRAIN 1 - 6/20018]: avg loss: 6.9622, avg time: 0.89s, ETA: 4h58m27s \n",
      "batch_loss = 7.0313864f0\n",
      "[TRAIN 1 - 7/20018]: avg loss: 6.9721, avg time: 0.91s, ETA: 5h4m52s \n",
      "batch_loss = 7.167213f0\n",
      "[TRAIN 1 - 8/20018]: avg loss: 6.9965, avg time: 0.93s, ETA: 5h11m43s \n",
      "batch_loss = 7.07827f0\n",
      "[TRAIN 1 - 9/20018]: avg loss: 7.0056, avg time: 0.96s, ETA: 5h19m8s \n",
      "batch_loss = 7.0133095f0\n",
      "[TRAIN 1 - 10/20018]: avg loss: 7.0063, avg time: 0.97s, ETA: 5h24m56s \n",
      "batch_loss = 7.131474f0\n",
      "[TRAIN 1 - 11/20018]: avg loss: 7.0177, avg time: 0.99s, ETA: 5h31m22s \n",
      "batch_loss = 7.1165767f0\n",
      "[TRAIN 1 - 12/20018]: avg loss: 7.0260, avg time: 1.01s, ETA: 5h38m7s \n",
      "batch_loss = 7.0434966f0\n",
      "[TRAIN 1 - 13/20018]: avg loss: 7.0273, avg time: 1.03s, ETA: 5h44m36s \n",
      "batch_loss = 7.341138f0\n",
      "[TRAIN 1 - 14/20018]: avg loss: 7.0497, avg time: 1.05s, ETA: 5h50m54s \n",
      "batch_loss = 7.0767326f0\n",
      "[TRAIN 1 - 15/20018]: avg loss: 7.0515, avg time: 1.07s, ETA: 5h56m41s \n",
      "batch_loss = 7.1909633f0\n",
      "[TRAIN 1 - 16/20018]: avg loss: 7.0602, avg time: 1.09s, ETA: 6h3m7s \n",
      "batch_loss = 7.6535316f0\n",
      "[TRAIN 1 - 17/20018]: avg loss: 7.0951, avg time: 1.11s, ETA: 6h9m3s \n",
      "batch_loss = 7.26151f0\n",
      "[TRAIN 1 - 18/20018]: avg loss: 7.1044, avg time: 1.13s, ETA: 6h15m1s \n",
      "batch_loss = 7.31052f0\n",
      "[TRAIN 1 - 19/20018]: avg loss: 7.1152, avg time: 1.14s, ETA: 6h20m56s \n",
      "batch_loss = 7.09128f0\n",
      "[TRAIN 1 - 20/20018]: avg loss: 7.1140, avg time: 1.16s, ETA: 6h26m55s \n",
      "batch_loss = 7.5812626f0\n",
      "[TRAIN 1 - 21/20018]: avg loss: 7.1363, avg time: 1.18s, ETA: 6h32m59s \n",
      "batch_loss = 7.6248775f0\n",
      "[TRAIN 1 - 22/20018]: avg loss: 7.1585, avg time: 1.20s, ETA: 6h39m0s \n",
      "batch_loss = 7.38128f0\n",
      "[TRAIN 1 - 23/20018]: avg loss: 7.1682, avg time: 1.21s, ETA: 6h44m49s \n",
      "batch_loss = 7.5851283f0\n",
      "[TRAIN 1 - 24/20018]: avg loss: 7.1856, avg time: 1.23s, ETA: 6h50m31s \n",
      "batch_loss = 7.520513f0\n",
      "[TRAIN 1 - 25/20018]: avg loss: 7.1989, avg time: 1.25s, ETA: 6h56m15s \n",
      "batch_loss = 7.662198f0\n",
      "[TRAIN 1 - 26/20018]: avg loss: 7.2168, avg time: 1.27s, ETA: 7h1m56s \n",
      "batch_loss = 7.373073f0\n",
      "[TRAIN 1 - 27/20018]: avg loss: 7.2226, avg time: 1.28s, ETA: 7h7m20s \n",
      "batch_loss = 7.65544f0\n",
      "[TRAIN 1 - 28/20018]: avg loss: 7.2380, avg time: 1.30s, ETA: 7h12m51s \n",
      "batch_loss = 7.401843f0\n",
      "[TRAIN 1 - 29/20018]: avg loss: 7.2437, avg time: 1.32s, ETA: 7h18m59s \n",
      "batch_loss = 7.541626f0\n",
      "[TRAIN 1 - 30/20018]: avg loss: 7.2536, avg time: 1.33s, ETA: 7h24m20s \n",
      "batch_loss = 7.664078f0\n",
      "[TRAIN 1 - 31/20018]: avg loss: 7.2668, avg time: 1.35s, ETA: 7h29m23s \n",
      "batch_loss = 7.607012f0\n",
      "[TRAIN 1 - 32/20018]: avg loss: 7.2775, avg time: 1.37s, ETA: 7h34m48s \n",
      "batch_loss = 7.557599f0\n",
      "[TRAIN 1 - 33/20018]: avg loss: 7.2860, avg time: 1.38s, ETA: 7h39m22s \n",
      "batch_loss = 7.889821f0\n",
      "[TRAIN 1 - 34/20018]: avg loss: 7.3037, avg time: 1.40s, ETA: 7h45m28s \n",
      "batch_loss = 7.570457f0\n",
      "[TRAIN 1 - 35/20018]: avg loss: 7.3113, avg time: 1.41s, ETA: 7h49m53s \n",
      "batch_loss = 7.829129f0\n",
      "[TRAIN 1 - 36/20018]: avg loss: 7.3257, avg time: 1.43s, ETA: 7h54m51s \n",
      "batch_loss = 7.7865896f0\n",
      "[TRAIN 1 - 37/20018]: avg loss: 7.3382, avg time: 1.44s, ETA: 8h0m39s \n",
      "batch_loss = 7.33513f0\n",
      "[TRAIN 1 - 38/20018]: avg loss: 7.3381, avg time: 1.46s, ETA: 8h5m11s \n",
      "batch_loss = 7.655512f0\n",
      "[TRAIN 1 - 39/20018]: avg loss: 7.3462, avg time: 1.47s, ETA: 8h9m59s \n",
      "batch_loss = 7.372562f0\n",
      "[TRAIN 1 - 40/20018]: avg loss: 7.3469, avg time: 1.49s, ETA: 8h14m58s \n",
      "batch_loss = 7.5467167f0\n",
      "[TRAIN 1 - 41/20018]: avg loss: 7.3518, avg time: 1.50s, ETA: 8h20m30s \n",
      "batch_loss = 7.5951633f0\n",
      "[TRAIN 1 - 42/20018]: avg loss: 7.3576, avg time: 1.52s, ETA: 8h24m37s \n",
      "batch_loss = 7.5205526f0\n",
      "[TRAIN 1 - 43/20018]: avg loss: 7.3614, avg time: 1.53s, ETA: 8h29m14s \n",
      "batch_loss = 7.2018003f0\n",
      "[TRAIN 1 - 44/20018]: avg loss: 7.3577, avg time: 1.55s, ETA: 8h34m28s \n",
      "batch_loss = 7.783224f0\n",
      "[TRAIN 1 - 45/20018]: avg loss: 7.3672, avg time: 1.56s, ETA: 8h40m44s \n",
      "batch_loss = 7.7686825f0\n",
      "[TRAIN 1 - 46/20018]: avg loss: 7.3759, avg time: 1.58s, ETA: 8h44m51s \n",
      "batch_loss = 7.1020703f0\n",
      "[TRAIN 1 - 47/20018]: avg loss: 7.3701, avg time: 1.59s, ETA: 8h49m16s \n",
      "batch_loss = 7.523679f0\n",
      "[TRAIN 1 - 48/20018]: avg loss: 7.3733, avg time: 1.60s, ETA: 8h53m45s \n",
      "batch_loss = 7.7012f0\n",
      "[TRAIN 1 - 49/20018]: avg loss: 7.3800, avg time: 1.62s, ETA: 8h59m49s \n",
      "batch_loss = 7.3365755f0\n",
      "[TRAIN 1 - 50/20018]: avg loss: 7.3791, avg time: 1.63s, ETA: 9h3m13s \n",
      "batch_loss = 7.137593f0\n",
      "[TRAIN 1 - 51/20018]: avg loss: 7.3744, avg time: 1.64s, ETA: 9h7m4s \n",
      "batch_loss = 6.7748346f0\n",
      "[TRAIN 1 - 52/20018]: avg loss: 7.3707, avg time: 1.66s, ETA: 9h11m38s \n",
      "batch_loss = 7.6484156f0\n",
      "[TRAIN 1 - 53/20018]: avg loss: 7.3848, avg time: 1.67s, ETA: 9h15m22s \n",
      "batch_loss = 7.461233f0\n",
      "[TRAIN 1 - 54/20018]: avg loss: 7.3950, avg time: 1.69s, ETA: 9h20m60s \n",
      "batch_loss = 7.0180135f0\n",
      "[TRAIN 1 - 55/20018]: avg loss: 7.3958, avg time: 1.70s, ETA: 9h24m21s \n",
      "batch_loss = 7.2069693f0\n",
      "[TRAIN 1 - 56/20018]: avg loss: 7.4016, avg time: 1.71s, ETA: 9h29m58s \n",
      "batch_loss = 7.3503304f0\n",
      "[TRAIN 1 - 57/20018]: avg loss: 7.4074, avg time: 1.72s, ETA: 9h33m14s \n",
      "batch_loss = 7.1234055f0\n",
      "[TRAIN 1 - 58/20018]: avg loss: 7.4092, avg time: 1.74s, ETA: 9h37m28s \n",
      "batch_loss = 7.170315f0\n",
      "[TRAIN 1 - 59/20018]: avg loss: 7.4093, avg time: 1.75s, ETA: 9h41m1s \n",
      "batch_loss = 7.1656704f0\n",
      "[TRAIN 1 - 60/20018]: avg loss: 7.4110, avg time: 1.76s, ETA: 9h44m48s \n",
      "batch_loss = 6.9516582f0\n",
      "[TRAIN 1 - 61/20018]: avg loss: 7.4098, avg time: 1.77s, ETA: 9h48m26s \n",
      "batch_loss = 7.330106f0\n",
      "[TRAIN 1 - 62/20018]: avg loss: 7.4137, avg time: 1.78s, ETA: 9h52m29s \n",
      "batch_loss = 6.720178f0\n",
      "[TRAIN 1 - 63/20018]: avg loss: 7.4059, avg time: 1.79s, ETA: 9h56m1s \n",
      "batch_loss = 7.1089444f0\n",
      "[TRAIN 1 - 64/20018]: avg loss: 7.4072, avg time: 1.80s, ETA: 9h59m55s \n",
      "batch_loss = 6.7845163f0\n",
      "[TRAIN 1 - 65/20018]: avg loss: 7.3963, avg time: 1.81s, ETA: 10h3m14s \n",
      "batch_loss = 7.158451f0\n",
      "[TRAIN 1 - 66/20018]: avg loss: 7.3979, avg time: 1.83s, ETA: 10h7m1s \n",
      "batch_loss = 6.865749f0\n",
      "[TRAIN 1 - 67/20018]: avg loss: 7.3915, avg time: 1.84s, ETA: 10h10m22s \n",
      "batch_loss = 6.91348f0\n",
      "[TRAIN 1 - 68/20018]: avg loss: 7.3770, avg time: 1.85s, ETA: 10h14m7s \n",
      "batch_loss = 6.7564955f0\n",
      "[TRAIN 1 - 69/20018]: avg loss: 7.3671, avg time: 1.86s, ETA: 10h17m30s \n",
      "batch_loss = 7.2078257f0\n",
      "[TRAIN 1 - 70/20018]: avg loss: 7.3651, avg time: 1.87s, ETA: 10h20m52s \n",
      "batch_loss = 6.5676007f0\n",
      "[TRAIN 1 - 71/20018]: avg loss: 7.3548, avg time: 1.88s, ETA: 10h23m54s \n",
      "batch_loss = 6.5625496f0\n",
      "[TRAIN 1 - 72/20018]: avg loss: 7.3349, avg time: 1.89s, ETA: 10h27m41s \n",
      "batch_loss = 6.7674713f0\n",
      "[TRAIN 1 - 73/20018]: avg loss: 7.3180, avg time: 1.90s, ETA: 10h31m52s \n",
      "batch_loss = 6.7032466f0\n",
      "[TRAIN 1 - 74/20018]: avg loss: 7.3048, avg time: 1.91s, ETA: 10h34m51s \n",
      "batch_loss = 6.949676f0\n",
      "[TRAIN 1 - 75/20018]: avg loss: 7.2923, avg time: 1.92s, ETA: 10h37m46s \n",
      "batch_loss = 6.8355093f0\n",
      "[TRAIN 1 - 76/20018]: avg loss: 7.2789, avg time: 1.93s, ETA: 10h41m9s \n",
      "batch_loss = 6.761684f0\n",
      "[TRAIN 1 - 77/20018]: avg loss: 7.2612, avg time: 1.94s, ETA: 10h44m10s \n",
      "batch_loss = 6.742987f0\n",
      "[TRAIN 1 - 78/20018]: avg loss: 7.2489, avg time: 1.95s, ETA: 10h47m23s \n",
      "batch_loss = 6.8627687f0\n",
      "[TRAIN 1 - 79/20018]: avg loss: 7.2333, avg time: 1.96s, ETA: 10h50m39s \n",
      "batch_loss = 6.658802f0\n",
      "[TRAIN 1 - 80/20018]: avg loss: 7.2187, avg time: 1.97s, ETA: 10h53m57s \n",
      "batch_loss = 6.874971f0\n",
      "[TRAIN 1 - 81/20018]: avg loss: 7.2057, avg time: 1.98s, ETA: 10h57m41s \n",
      "batch_loss = 6.808718f0\n",
      "[TRAIN 1 - 82/20018]: avg loss: 7.1889, avg time: 1.99s, ETA: 11h0m38s \n",
      "batch_loss = 6.615429f0\n",
      "[TRAIN 1 - 83/20018]: avg loss: 7.1695, avg time: 2.00s, ETA: 11h3m33s \n",
      "batch_loss = 6.7408957f0\n",
      "[TRAIN 1 - 84/20018]: avg loss: 7.1534, avg time: 2.01s, ETA: 11h6m36s \n",
      "batch_loss = 6.48777f0\n",
      "[TRAIN 1 - 85/20018]: avg loss: 7.1259, avg time: 2.02s, ETA: 11h11m55s \n",
      "batch_loss = 6.7756104f0\n",
      "[TRAIN 1 - 86/20018]: avg loss: 7.1104, avg time: 2.03s, ETA: 11h14m22s \n",
      "batch_loss = 6.249303f0\n",
      "[TRAIN 1 - 87/20018]: avg loss: 7.0794, avg time: 2.04s, ETA: 11h17m3s \n",
      "batch_loss = 6.717087f0\n",
      "[TRAIN 1 - 88/20018]: avg loss: 7.0584, avg time: 2.05s, ETA: 11h19m48s \n",
      "batch_loss = 6.4166083f0\n",
      "[TRAIN 1 - 89/20018]: avg loss: 7.0404, avg time: 2.05s, ETA: 11h22m20s \n",
      "batch_loss = 6.710187f0\n",
      "[TRAIN 1 - 90/20018]: avg loss: 7.0219, avg time: 2.06s, ETA: 11h25m50s \n",
      "batch_loss = 6.438019f0\n",
      "[TRAIN 1 - 91/20018]: avg loss: 7.0035, avg time: 2.07s, ETA: 11h28m9s \n",
      "batch_loss = 6.7583857f0\n",
      "[TRAIN 1 - 92/20018]: avg loss: 6.9881, avg time: 2.08s, ETA: 11h31m18s \n",
      "batch_loss = 6.522828f0\n",
      "[TRAIN 1 - 93/20018]: avg loss: 6.9671, avg time: 2.09s, ETA: 11h33m55s \n",
      "batch_loss = 6.576439f0\n",
      "[TRAIN 1 - 94/20018]: avg loss: 6.9485, avg time: 2.10s, ETA: 11h36m43s \n",
      "batch_loss = 6.5601664f0\n",
      "[TRAIN 1 - 95/20018]: avg loss: 6.9360, avg time: 2.11s, ETA: 11h39m28s \n",
      "batch_loss = 6.541833f0\n",
      "[TRAIN 1 - 96/20018]: avg loss: 6.9116, avg time: 2.12s, ETA: 11h42m38s \n",
      "batch_loss = 6.7442126f0\n",
      "[TRAIN 1 - 97/20018]: avg loss: 6.8915, avg time: 2.13s, ETA: 11h45m54s \n",
      "batch_loss = 7.0219116f0\n",
      "[TRAIN 1 - 98/20018]: avg loss: 6.8900, avg time: 2.14s, ETA: 11h48m49s \n",
      "batch_loss = 6.8299356f0\n",
      "[TRAIN 1 - 99/20018]: avg loss: 6.8764, avg time: 2.14s, ETA: 11h51m58s \n",
      "batch_loss = 6.6847277f0\n",
      "[TRAIN 1 - 100/20018]: avg loss: 6.8564, avg time: 2.15s, ETA: 11h54m59s \n",
      "batch_loss = 6.6075153f0\n",
      "[TRAIN 1 - 101/20018]: avg loss: 6.8421, avg time: 2.16s, ETA: 11h57m14s \n",
      "batch_loss = 6.675604f0\n",
      "[TRAIN 1 - 102/20018]: avg loss: 6.8331, avg time: 2.17s, ETA: 11h59m33s \n",
      "batch_loss = 6.882872f0\n",
      "[TRAIN 1 - 103/20018]: avg loss: 6.8352, avg time: 2.18s, ETA: 12h1m57s \n",
      "batch_loss = 6.5191336f0\n",
      "[TRAIN 1 - 104/20018]: avg loss: 6.8131, avg time: 2.18s, ETA: 12h4m16s \n",
      "batch_loss = 6.5079465f0\n",
      "[TRAIN 1 - 105/20018]: avg loss: 6.7944, avg time: 2.19s, ETA: 12h6m27s \n",
      "batch_loss = 6.1698112f0\n",
      "[TRAIN 1 - 106/20018]: avg loss: 6.7777, avg time: 2.20s, ETA: 12h10m40s \n",
      "batch_loss = 6.469122f0\n",
      "[TRAIN 1 - 107/20018]: avg loss: 6.7633, avg time: 2.21s, ETA: 12h12m9s \n",
      "batch_loss = 6.3438797f0\n",
      "[TRAIN 1 - 108/20018]: avg loss: 6.7435, avg time: 2.21s, ETA: 12h14m53s \n",
      "batch_loss = 6.2818766f0\n",
      "[TRAIN 1 - 109/20018]: avg loss: 6.7270, avg time: 2.22s, ETA: 12h16m52s \n",
      "batch_loss = 6.815174f0\n",
      "[TRAIN 1 - 110/20018]: avg loss: 6.7201, avg time: 2.23s, ETA: 12h19m50s \n",
      "batch_loss = 6.5535192f0\n",
      "[TRAIN 1 - 111/20018]: avg loss: 6.7081, avg time: 2.24s, ETA: 12h22m45s \n",
      "batch_loss = 6.1814556f0\n",
      "[TRAIN 1 - 112/20018]: avg loss: 6.6930, avg time: 2.25s, ETA: 12h26m41s \n",
      "batch_loss = 6.302928f0\n",
      "[TRAIN 1 - 113/20018]: avg loss: 6.6728, avg time: 2.26s, ETA: 12h30m22s \n",
      "batch_loss = 6.178856f0\n",
      "[TRAIN 1 - 114/20018]: avg loss: 6.6622, avg time: 2.27s, ETA: 12h31m30s \n",
      "batch_loss = 6.2830343f0\n",
      "[TRAIN 1 - 115/20018]: avg loss: 6.6460, avg time: 2.27s, ETA: 12h33m31s \n",
      "batch_loss = 6.4876084f0\n",
      "[TRAIN 1 - 116/20018]: avg loss: 6.6402, avg time: 2.28s, ETA: 12h35m40s \n",
      "batch_loss = 6.1451254f0\n",
      "[TRAIN 1 - 117/20018]: avg loss: 6.6203, avg time: 2.29s, ETA: 12h39m5s \n",
      "batch_loss = 6.297739f0\n",
      "[TRAIN 1 - 118/20018]: avg loss: 6.6092, avg time: 2.29s, ETA: 12h40m36s \n",
      "batch_loss = 6.553295f0\n",
      "[TRAIN 1 - 119/20018]: avg loss: 6.6021, avg time: 2.30s, ETA: 12h42m14s \n",
      "batch_loss = 6.344301f0\n",
      "[TRAIN 1 - 120/20018]: avg loss: 6.5940, avg time: 2.31s, ETA: 12h44m49s \n",
      "batch_loss = 6.113551f0\n",
      "[TRAIN 1 - 121/20018]: avg loss: 6.5726, avg time: 2.31s, ETA: 12h46m12s \n",
      "batch_loss = 6.2233915f0\n",
      "[TRAIN 1 - 122/20018]: avg loss: 6.5658, avg time: 2.32s, ETA: 12h48m17s \n",
      "batch_loss = 6.2611136f0\n",
      "[TRAIN 1 - 123/20018]: avg loss: 6.5599, avg time: 2.33s, ETA: 12h51m3s \n",
      "batch_loss = 6.4684725f0\n",
      "[TRAIN 1 - 124/20018]: avg loss: 6.5541, avg time: 2.33s, ETA: 12h52m31s \n",
      "batch_loss = 6.402356f0\n",
      "[TRAIN 1 - 125/20018]: avg loss: 6.5482, avg time: 2.33s, ETA: 12h54m4s \n",
      "batch_loss = 6.0890493f0\n",
      "[TRAIN 1 - 126/20018]: avg loss: 6.5313, avg time: 2.34s, ETA: 12h56m16s \n",
      "batch_loss = 6.241959f0\n",
      "[TRAIN 1 - 127/20018]: avg loss: 6.5196, avg time: 2.35s, ETA: 12h57m48s \n",
      "batch_loss = 6.143773f0\n",
      "[TRAIN 1 - 128/20018]: avg loss: 6.5075, avg time: 2.36s, ETA: 13h2m30s \n",
      "batch_loss = 6.28998f0\n",
      "[TRAIN 1 - 129/20018]: avg loss: 6.4986, avg time: 2.36s, ETA: 13h2m27s \n",
      "batch_loss = 6.2962484f0\n",
      "[TRAIN 1 - 130/20018]: avg loss: 6.4875, avg time: 2.37s, ETA: 13h5m14s \n",
      "batch_loss = 6.232935f0\n",
      "[TRAIN 1 - 131/20018]: avg loss: 6.4792, avg time: 2.38s, ETA: 13h7m51s \n",
      "batch_loss = 6.422283f0\n",
      "[TRAIN 1 - 132/20018]: avg loss: 6.4703, avg time: 2.38s, ETA: 13h10m4s \n",
      "batch_loss = 6.2529445f0\n",
      "[TRAIN 1 - 133/20018]: avg loss: 6.4594, avg time: 2.39s, ETA: 13h12m41s \n",
      "batch_loss = 6.376482f0\n",
      "[TRAIN 1 - 134/20018]: avg loss: 6.4547, avg time: 2.40s, ETA: 13h14m32s \n",
      "batch_loss = 6.529128f0\n",
      "[TRAIN 1 - 135/20018]: avg loss: 6.4506, avg time: 2.41s, ETA: 13h17m16s \n",
      "batch_loss = 6.2505274f0\n",
      "[TRAIN 1 - 136/20018]: avg loss: 6.4459, avg time: 2.41s, ETA: 13h19m23s \n",
      "batch_loss = 6.1326075f0\n",
      "[TRAIN 1 - 137/20018]: avg loss: 6.4333, avg time: 2.42s, ETA: 13h20m57s \n",
      "batch_loss = 6.0665383f0\n",
      "[TRAIN 1 - 138/20018]: avg loss: 6.4297, avg time: 2.42s, ETA: 13h23m12s \n",
      "batch_loss = 6.494875f0\n",
      "[TRAIN 1 - 139/20018]: avg loss: 6.4254, avg time: 2.43s, ETA: 13h24m47s \n",
      "batch_loss = 6.2976084f0\n",
      "[TRAIN 1 - 140/20018]: avg loss: 6.4230, avg time: 2.44s, ETA: 13h26m57s \n",
      "batch_loss = 6.24177f0\n",
      "[TRAIN 1 - 141/20018]: avg loss: 6.4139, avg time: 2.44s, ETA: 13h29m2s \n",
      "batch_loss = 6.016088f0\n",
      "[TRAIN 1 - 142/20018]: avg loss: 6.4056, avg time: 2.45s, ETA: 13h31m12s \n",
      "batch_loss = 6.215699f0\n",
      "[TRAIN 1 - 143/20018]: avg loss: 6.3949, avg time: 2.45s, ETA: 13h32m54s \n",
      "batch_loss = 6.331113f0\n",
      "[TRAIN 1 - 144/20018]: avg loss: 6.3912, avg time: 2.46s, ETA: 13h35m18s \n",
      "batch_loss = 6.5089054f0\n",
      "[TRAIN 1 - 145/20018]: avg loss: 6.3899, avg time: 2.47s, ETA: 13h36m50s \n",
      "batch_loss = 5.943267f0\n",
      "[TRAIN 1 - 146/20018]: avg loss: 6.3778, avg time: 2.47s, ETA: 13h38m29s \n",
      "batch_loss = 6.3464794f0\n",
      "[TRAIN 1 - 147/20018]: avg loss: 6.3739, avg time: 2.48s, ETA: 13h39m48s \n",
      "batch_loss = 6.5075245f0\n",
      "[TRAIN 1 - 148/20018]: avg loss: 6.3693, avg time: 2.48s, ETA: 13h41m38s \n",
      "batch_loss = 5.843158f0\n",
      "[TRAIN 1 - 149/20018]: avg loss: 6.3462, avg time: 2.49s, ETA: 13h42m58s \n",
      "batch_loss = 6.015974f0\n",
      "[TRAIN 1 - 150/20018]: avg loss: 6.3302, avg time: 2.49s, ETA: 13h44m59s \n",
      "batch_loss = 6.2874823f0\n",
      "[TRAIN 1 - 151/20018]: avg loss: 6.3224, avg time: 2.50s, ETA: 13h47m46s \n",
      "batch_loss = 6.145458f0\n",
      "[TRAIN 1 - 152/20018]: avg loss: 6.3134, avg time: 2.51s, ETA: 13h49m56s \n",
      "batch_loss = 6.2152166f0\n",
      "[TRAIN 1 - 153/20018]: avg loss: 6.3043, avg time: 2.51s, ETA: 13h50m57s \n",
      "batch_loss = 6.4732656f0\n",
      "[TRAIN 1 - 154/20018]: avg loss: 6.2963, avg time: 2.52s, ETA: 13h53m8s \n",
      "batch_loss = 6.2842054f0\n",
      "[TRAIN 1 - 155/20018]: avg loss: 6.2917, avg time: 2.52s, ETA: 13h53m53s \n",
      "batch_loss = 6.099002f0\n",
      "[TRAIN 1 - 156/20018]: avg loss: 6.2837, avg time: 2.52s, ETA: 13h55m18s \n",
      "batch_loss = 6.0108767f0\n",
      "[TRAIN 1 - 157/20018]: avg loss: 6.2806, avg time: 2.53s, ETA: 13h56m17s \n",
      "batch_loss = 6.3169117f0\n",
      "[TRAIN 1 - 158/20018]: avg loss: 6.2776, avg time: 2.53s, ETA: 13h57m48s \n",
      "batch_loss = 6.130246f0\n",
      "[TRAIN 1 - 159/20018]: avg loss: 6.2734, avg time: 2.53s, ETA: 13h58m58s \n",
      "batch_loss = 6.179352f0\n",
      "[TRAIN 1 - 160/20018]: avg loss: 6.2714, avg time: 2.54s, ETA: 14h0m40s \n",
      "batch_loss = 6.1209974f0\n",
      "[TRAIN 1 - 161/20018]: avg loss: 6.2578, avg time: 2.54s, ETA: 14h1m52s \n",
      "batch_loss = 5.9963007f0\n",
      "[TRAIN 1 - 162/20018]: avg loss: 6.2469, avg time: 2.55s, ETA: 14h3m14s \n",
      "batch_loss = 6.0571094f0\n",
      "[TRAIN 1 - 163/20018]: avg loss: 6.2444, avg time: 2.55s, ETA: 14h5m8s \n",
      "batch_loss = 5.976975f0\n",
      "[TRAIN 1 - 164/20018]: avg loss: 6.2380, avg time: 2.56s, ETA: 14h6m28s \n",
      "batch_loss = 6.4343805f0\n",
      "[TRAIN 1 - 165/20018]: avg loss: 6.2430, avg time: 2.56s, ETA: 14h7m36s \n",
      "batch_loss = 6.3204055f0\n",
      "[TRAIN 1 - 166/20018]: avg loss: 6.2438, avg time: 2.57s, ETA: 14h9m1s \n",
      "batch_loss = 6.144717f0\n",
      "[TRAIN 1 - 167/20018]: avg loss: 6.2370, avg time: 2.57s, ETA: 14h10m15s \n",
      "batch_loss = 5.978823f0\n",
      "[TRAIN 1 - 168/20018]: avg loss: 6.2338, avg time: 2.58s, ETA: 14h12m2s \n",
      "batch_loss = 6.2657084f0\n",
      "[TRAIN 1 - 169/20018]: avg loss: 6.2332, avg time: 2.58s, ETA: 14h12m52s \n",
      "batch_loss = 6.108475f0\n",
      "[TRAIN 1 - 170/20018]: avg loss: 6.2244, avg time: 2.58s, ETA: 14h14m26s \n",
      "batch_loss = 6.2016907f0\n",
      "[TRAIN 1 - 171/20018]: avg loss: 6.2216, avg time: 2.59s, ETA: 14h15m34s \n",
      "batch_loss = 6.105574f0\n",
      "[TRAIN 1 - 172/20018]: avg loss: 6.2215, avg time: 2.60s, ETA: 14h19m12s \n",
      "batch_loss = 6.411059f0\n",
      "[TRAIN 1 - 173/20018]: avg loss: 6.2252, avg time: 2.60s, ETA: 14h18m48s \n",
      "batch_loss = 6.2743998f0\n",
      "[TRAIN 1 - 174/20018]: avg loss: 6.2254, avg time: 2.60s, ETA: 14h20m1s \n",
      "batch_loss = 6.207641f0\n",
      "[TRAIN 1 - 175/20018]: avg loss: 6.2203, avg time: 2.61s, ETA: 14h23m0s \n",
      "batch_loss = 6.2922597f0\n",
      "[TRAIN 1 - 176/20018]: avg loss: 6.2181, avg time: 2.61s, ETA: 14h24m6s \n",
      "batch_loss = 6.4356365f0\n",
      "[TRAIN 1 - 177/20018]: avg loss: 6.2249, avg time: 2.62s, ETA: 14h25m28s \n",
      "batch_loss = 6.0347633f0\n",
      "[TRAIN 1 - 178/20018]: avg loss: 6.2209, avg time: 2.62s, ETA: 14h27m38s \n",
      "batch_loss = 5.9667406f0\n",
      "[TRAIN 1 - 179/20018]: avg loss: 6.2174, avg time: 2.63s, ETA: 14h28m10s \n",
      "batch_loss = 5.7165823f0\n",
      "[TRAIN 1 - 180/20018]: avg loss: 6.2062, avg time: 2.63s, ETA: 14h29m41s \n",
      "batch_loss = 5.9902f0\n",
      "[TRAIN 1 - 181/20018]: avg loss: 6.2002, avg time: 2.63s, ETA: 14h30m57s \n",
      "batch_loss = 6.1820183f0\n",
      "[TRAIN 1 - 182/20018]: avg loss: 6.1992, avg time: 2.64s, ETA: 14h32m12s \n",
      "batch_loss = 5.860342f0\n",
      "[TRAIN 1 - 183/20018]: avg loss: 6.1881, avg time: 2.64s, ETA: 14h33m3s \n",
      "batch_loss = 5.9650106f0\n",
      "[TRAIN 1 - 184/20018]: avg loss: 6.1825, avg time: 2.64s, ETA: 14h34m7s \n",
      "batch_loss = 6.3115206f0\n",
      "[TRAIN 1 - 185/20018]: avg loss: 6.1812, avg time: 2.65s, ETA: 14h34m42s \n",
      "batch_loss = 6.3297844f0\n",
      "[TRAIN 1 - 186/20018]: avg loss: 6.1773, avg time: 2.65s, ETA: 14h36m2s \n",
      "batch_loss = 6.3033185f0\n",
      "[TRAIN 1 - 187/20018]: avg loss: 6.1784, avg time: 2.66s, ETA: 14h37m58s \n",
      "batch_loss = 5.835417f0\n",
      "[TRAIN 1 - 188/20018]: avg loss: 6.1725, avg time: 2.66s, ETA: 14h38m53s \n",
      "batch_loss = 6.0861673f0\n",
      "[TRAIN 1 - 189/20018]: avg loss: 6.1729, avg time: 2.66s, ETA: 14h39m37s \n"
     ]
    }
   ],
   "source": [
    "# Train away, train away, train away |> 's/train/sail/ig'\n",
    "@info(\"Beginning training run...\")\n",
    "train(model, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
